{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skills from resume.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a5i3aVJCw4iG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-WC0Et3a_Za"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "15IH3oYhpggM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "xb7PoZFgbGRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "o5EqecC7bKjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Sample Resume"
      ],
      "metadata": {
        "id": "SMHiV3JhpzEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  fitz\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words(\"english\"))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "fname = \"/content/drive/MyDrive/Resume/Ankit_Gupta_Resume.pdf\"  # get document filename\n",
        "with fitz.open(fname) as doc:\n",
        "   text = \"\"\n",
        "   for page in doc:\n",
        "        text += page.getText()\n",
        "#print(text)\n",
        "#character_only_text=re.sub(\"[^a-zA-Z0-9@]\",\" \",text)\n",
        "character_only_text=re.sub(\"[( , )]\",\" \",text)\n",
        "    \n",
        "    \n",
        "    # Lowercase and split\n",
        "#lower_text=character_only_text.lower().split()\n",
        "    \n",
        "    #Get STOPWORDS and remove\n",
        "stop_remove_text=[i for i in lower_text if not i in stopwords]\n",
        "\n",
        "    \n",
        "    #Lemmatization\n",
        "lemma_removed_text=[wordnet_lemmatizer.lemmatize(word,'v') for word in stop_remove_text]\n",
        "    \n",
        "    \n",
        "final_text= \" \".join(stop_remove_text)\n",
        "print(\"*\"*50)\n",
        "print(final_text)"
      ],
      "metadata": {
        "id": "U_vOUjKiccPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Email and phone number using regular expression"
      ],
      "metadata": {
        "id": "MJ_0VZ0VtIVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email=re.findall('\\w*@[a-z]*.[a-z]*',text)\n",
        "print(email)\n",
        "contact_number=re.findall('\\d{10}',text)\n",
        "print(contact_number)"
      ],
      "metadata": {
        "id": "VXEECyKItExo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Custom Entity model for SKills"
      ],
      "metadata": {
        "id": "FHmbXmiyqGmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import minibatch,compounding\n",
        "import random\n",
        "LABEL = \"SKILLS\"\n",
        "\n",
        "TRAIN_DATA = [\n",
        "      (\"Java are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 4, LABEL)]},\n",
        "    ),\n",
        "    (\"Do they bite?\", {\"entities\": []}),\n",
        "    (\n",
        "        \"reactJS are too tall and they pretend to care about your feelings\",\n",
        "        {\"entities\": [(0, 7, LABEL)]},\n",
        "    ),\n",
        "    (\"HTML pretend to care about your feelings\", {\"entities\": [(0, 4, LABEL)]}),\n",
        "    (\n",
        "        \"they pretend to care about your feelings, those github\",\n",
        "        {\"entities\": [(48, 54, LABEL)]},\n",
        "    ),\n",
        "    (\"Programming Language?\", {\"entities\": [(0, 20, LABEL)]}),\n",
        "\n",
        "    (\"Software Development is my passion\",{\"entities\":[(0,20,LABEL)]}),\n",
        "              (\"Programming Language is my skill\",{\"entities\":[(0,20,LABEL)]}),\n",
        "              (\"c is middel level language\",{\"entities\":[(0,1,LABEL)]}),\n",
        "              (\"Bootstraping \",{\"entities\":[(0,12,LABEL)]}),\n",
        "              (\"Github is for saving my repo\",{\"entities\":[(0,6,LABEL)]}),\n",
        "              (\"reactjs is for app development\",{\"entities\":[(0,7,LABEL)]}),\n",
        "              (\"database is used to store data\",{\"entities\":[(0,8,LABEL)]})\n",
        "]\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "nlp.add_pipe(ner)\n",
        "\n",
        "ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "move_names = list(ner.move_names)\n",
        "# get names of other pipes to disable them during training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    sizes = compounding(1.0, 4.0, 1.001)\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    for itn in range(30):\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "        print(\"Losses\", losses)"
      ],
      "metadata": {
        "id": "C3LOPzazAjcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Enity pipe Model"
      ],
      "metadata": {
        "id": "D_PYn55Or1ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "output_dir=Path('/content/drive/MyDrive/Resume/output_dir')\n",
        "if not output_dir.exists():\n",
        "  output_dir.mkdir()\n",
        "nlp.meta['name']='My_ner'\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"saved model to \",output_dir)\n"
      ],
      "metadata": {
        "id": "pq8L3c8vO7c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Custom Enity Pipe for testing"
      ],
      "metadata": {
        "id": "gn3WS7y6r9A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2=spacy.load(output_dir)\n",
        "assert nlp2.get_pipe(\"ner\")#.move_names == move_names\n",
        "doc2=nlp2(text)\n",
        "#for ent in doc2.ents:\n",
        " # print(ent.label_,ent.text)"
      ],
      "metadata": {
        "id": "U5P8Gi-INLAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Custom Entity Pipe in Spacy Model"
      ],
      "metadata": {
        "id": "qAcU26Emxr1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "#spacy.load(output_dir)\n",
        "custom_nlp = spacy.load(output_dir, vocab=nlp.vocab)\n",
        "nlp.add_pipe(custom_nlp.get_pipe(\"ner\"), name=\"my_ner\", before=\"ner\")"
      ],
      "metadata": {
        "id": "5KCsE9-ZSbwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Extracting Technical Skills"
      ],
      "metadata": {
        "id": "QkRGn-7osZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tskills=list()\n",
        "tx=nlp(text)\n",
        "i=0\n",
        "for l in tx.ents:\n",
        "   if l.label_ == 'SKILLS' :\n",
        "      tskills.append(l.text)\n",
        "      print(l.text,\"->\",l.label_)\n",
        "      "
      ],
      "metadata": {
        "id": "GS8x0SMCSxee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tskills)"
      ],
      "metadata": {
        "id": "OW1AtzDSYiDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(tx, style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "q9ta6tDjsxhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting non technical Skills"
      ],
      "metadata": {
        "id": "Mlsu8M0Ksf2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NTskills=list()\n",
        "tx=nlp(final_text)\n",
        "#print(tx.ents)\n",
        "#sente = list(tx.sents)\n",
        "#for s in sente:\n",
        "  #print(s.ents)\n",
        "  #i=0\n",
        "for l in tx.ents:\n",
        "   if l.label_ == ' WORK_OF_ART' or l.label_ == 'ORG' or l.label_ == 'PERSON'  :\n",
        "      NTskills.append(l.text)\n",
        "      print(l.text,\"->\",l.label_)"
      ],
      "metadata": {
        "id": "MU_nhG7eTzYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(NTskills)"
      ],
      "metadata": {
        "id": "EpuByXtjYHrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(tx, style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "6Wgr-r4GrS0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting data to Excel"
      ],
      "metadata": {
        "id": "QNgb9f5iwhI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfts = pd.DataFrame()\n",
        "dfnts=pd.DataFrame()"
      ],
      "metadata": {
        "id": "0qpEwv3jtlcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfts['skill']=tskills[0:]\n",
        "dfnts['non tec skill']=NTskills[0:]"
      ],
      "metadata": {
        "id": "Jck-B03rttT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfts.to_excel('/content/drive/MyDrive/Resume/techskills.xlsx', index = False)\n",
        "dfnts.to_excel('/content/drive/MyDrive/Resume/nontechskills.xlsx', index = False)"
      ],
      "metadata": {
        "id": "O3oz5tf2uUj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tried Parts of Speeach for chunking "
      ],
      "metadata": {
        "id": "6PuPkzpEu81q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk\n",
        "#tx=\"ankit gupta indian institute information technology vadodara email ankitgupta8292@gmail com mob 8292858632 address vill post kohara bazar dist saran bihar 841205 skills area interest software development programming language c java web technologies html css js bootstrap4 tools github visual studio code framework library reactjs redux codeigniter database postgresql mysql education degree institute year cpi aggregate b tech indian institute information technology vadodara 2017 present 7 56 intermediate shakti shanti academy dighwara chhapra 2016 76 60 high school bhagwat vidyapeeth rushi chawni chhapra 2014 9 8 cgpa projects github finder web application reactjs guide self project users search github account see details like bio followers following last repos etc project made reactjs github link project deployed netlify link hotel rooms web application reactjs guide self hotel room project customers choose room according choice project various types rooms available like double deluxe room family deluxe room presindential room etc customer see also brief details room like price capacity person size room etc made project using react js bootstrap4 technologies project made front end part github link project deployed netlify link movie series application web application reactjs redux guide self application user search movie movie related query displayed page user see brief details movie displayed page build application using reactjs redux imdb api github link library management system web application codeigniter guide self project library management admin issue book currently available library authorized member library library management admin see book issued project using html css php codeigniter framework technologies database made mysql github link online railway reservation system guide prof pm jat daiict group project dbms based postgresql users book cancel ticket also see details ticket like date journey fare ticket source station destination station github link 1 awards achievements participation coding contest codechef udacity certi cate react nanodegree program link coursera certi cate hong kong university science technology completing front end web development react course link udemy certi cate completing react course link certi cate ice technology lab completing core java course github link personal traits highly motivated eager learn new things strong motivational leadership skills ability work individual well group interests activities playing watching cricket reading newspaper listening music declaration hereby declare details furnished true best knowledge belief\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for i in sentences:\n",
        "  word = nltk.word_tokenize(i)\n",
        "  tag = nltk.pos_tag(word)\n",
        " # NER=[ne_chunk(tag)]\n",
        "  grammer=\"NP: {<NN|NNP><NN/NNP>}\"\n",
        "  chunkph=nltk.RegexpParser(grammer)\n",
        "  res=chunkph.parse(tag)\n",
        "  print(res)\n",
        "#res.draw()\n",
        "\n",
        "'''wt=word_tokenize(tx)\n",
        "#tx=nlp(final_text)\n",
        "#print(tx)\n",
        "tags=nltk.pos_tag(wt)\n",
        "#print(tags)\n",
        "NER=[ne_chunk(sent) for sent in sentences]\n",
        "print(NER)''"
      ],
      "metadata": {
        "id": "1QjHJG7EutTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}